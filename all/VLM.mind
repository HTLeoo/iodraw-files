{"root":{"data":{"id":"deu7jy5tci00","created":1765336477861,"text":"VLM"},"children":[{"data":{"id":"deu7k7q3xhs0","created":1765336498679,"text":"load_model","layout_mind_offset":{"x":-38.99999941885471,"y":-172.99999742209926}},"children":[{"data":{"id":"deu7kf486c80","created":1765336514770,"text":"vision_backbone[dinov2+siglip],\nimage_transform[dinov2+siglip]","layout_right_offset":{"x":-1.9999999701976776,"y":-62.99999906122687}},"children":[{"data":{"id":"deubr9gqrrc0","created":1765348335455,"text":"dinov2-get_intermediate_layers(image, n=len(blocks)-2)，\nsiglip-与dinov2类似"},"children":[]}]},{"data":{"id":"deu7okdrm6o0","created":1765336839687,"text":"llm_backbone[Llama2-7B],\ntokenizer[]"},"children":[{"data":{"id":"deueflkpz4w0","created":1765355884776,"text":"Tokenizer将输入转化成整数idx,训练需要mask掉后文，预测时则逐token预测，直到达到最大长度或者eos?"},"children":[]}]},{"data":{"id":"deu87ajchmo0","created":1765338307176,"text":"vlm[vision backbone+llm_backbone+projector]"},"children":[]}]},{"data":{"id":"deu7k99n37k0","created":1765336502037,"text":"分支主题","layout_mind_offset":{"x":404.9999939650298,"y":109.99999836087221}},"children":[]},{"data":{"id":"deyksdmqg0w0","created":1765780052704,"text":"LLM","layout_mind_offset":{"x":116.16665345968556,"y":-184.8333305315012}},"children":[{"data":{"id":"deyksjbzkk80","created":1765780065115,"text":"Llama","expandState":"expand"},"children":[{"data":{"id":"deyksmrz4rs0","created":1765780072612,"text":"Tokenizer-"},"children":[{"data":{"id":"deykv3z2teg0","created":1765780266775,"text":"n_words词库大小\nBOS开始id\nEOS结束id\npadding 填充id","layout_left_offset":{"x":-499.83330385428496,"y":15.83333413013139}},"children":[]},{"data":{"id":"deykw3jnsc80","created":1765780344207,"text":"encode-负责将任意输入转成纯int的id列表，\nencode包含BOS,EOS两个flag，以确认是否在最前面或者最后加上对应的id。\ndecode-将纯int的id列表转换成文本","layout_left_offset":{"x":-207.49999086683044,"y":26.66666461361791}},"children":[]}]},{"data":{"id":"deykxz73wao0","created":1765780491469,"text":"Transformer"},"children":[{"data":{"id":"deyky5a5bo80","created":1765780504714,"text":"embedding-接受vocab_size和embed_dim两个参数，表示维护的词表大小和维度；\n输入是[bsz, len]的token，然后根据词表找到对应的token,得到结果就是[bsz,len,embed_dim]的embeddings","layout_left_offset":{"x":-130.00001202854696,"y":45.666663953827594}},"children":[]},{"data":{"id":"deylixe8eiw0","created":1765782133194,"text":"mask_attention:这里主要接受idx，和mask;\n会额外构造cache-k,cache-v\n推理时，在生成qkv后，对qk进行rope位置编码；\ncache每次只更新最近固定长度的内容；但是kv是逐步增长的；q会根据这个不断增长的k,v生成attention，最后得到结果。","layout_left_offset":{"x":-172.4999841488908,"y":38.333329810864655}},"children":[]},{"data":{"id":"deylzve3q4o0","created":1765783461023,"text":"output，是一个linear层，只对最后一个token计算；\n输入维度是dim，输出维度是vocab_size；\n最后这个是一个类似于分类的映射，找到词苦每个词对应的概率值","layout_left_offset":{"x":-215.83331350030255,"y":73.3333265946975}},"children":[]}]},{"data":{"id":"deymh05mi540","created":1765784803585,"text":"generate:\n现将长度不一致的prompts都转换成tokens,然后为了batch操作使用tokenizer.pad_id进行填充；\n推理时，第一步是至少一个prompt的最短的那个位置作为cur_pos,然后model.forward(tks[:,prev:cur,:],cur);\noutput通过top-p采样得到next token;\n每一步只包含next token或者token[cur_pos],取决这个是否已经把prompt加载完了，没有加载完就用token，加载完就是next token；\n终止条件是，要么是达到模型本身的max_len;要么是用户定义的max_len+prompt_len;\n最后用tokenizer逐步解码即可连成回答。","layout_left_offset":{"x":18.33333204603889,"y":64.33332600444635}},"children":[]}]}]},{"data":{"id":"dez8wrfws280","created":1765848102861,"text":"visual encoder"},"children":[]}]},"template":"default","theme":"fresh-blue","version":"1.4.43"}